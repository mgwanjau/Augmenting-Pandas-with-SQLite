{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qadTA4HuceEl",
        "NOEk3_YPdg6p",
        "OA2D7o4sds1t",
        "Rt1jPEhseD8r"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZK7M8-wZ_J8"
      },
      "source": [
        "# Project Notebook: Augmenting Pandas with SQLite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qadTA4HuceEl"
      },
      "source": [
        "## Question 1: Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF9k8rixclSR"
      },
      "source": [
        "In this session, we explored a few different ways to work with larger datasets in pandas. In this guided project, we'll practice using some of the techniques we learned to analyze startup investments from Crunchbase.com.\n",
        "\n",
        "Every year, thousands of startup companies raise financing from investors. Each time a startup raises money, we refer to the event as a fundraising round. Crunchbase is a website that crowdsources information on the fundraising rounds of many startups. The Crunchbase user community submits, edits, and maintains most of the information in Crunchbase.\n",
        "\n",
        "In return, Crunchbase makes the data available through a Web application and a fee-based API. Before Crunchbase switched to the paid API model, multiple groups crawled the site and released the data online. Because the information on the startups and their fundraising rounds is always changing, the data set we'll be using isn't completely up to date.\n",
        "\n",
        "Throughout this project, we'll practice working with different memory constraints. In this step, let's assume we only have 10 megabytes of available memory. While crunchbase-investments.csv (https://bit.ly/3BPcobU) consumes 10.3 megabytes of disk space, we know from earlier lessons that pandas often requires 4 to 6 times amount of space in memory as the file does on disk (especially when there's many string columns).\n",
        "\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "* Because the data set contains over 50,000 rows, you'll need to read the data set into dataframes using 5,000 row chunks to ensure that each chunk consumes much less than 10 megabytes of memory.\n",
        "* Across all of the chunks, become familiar with:\n",
        "1. Each column's missing value counts.\n",
        "2. Each column's memory footprint.\n",
        "3. The total memory footprint of all of the chunks combined.\n",
        "4. Which column(s) we can drop because they aren't useful for analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOYoNl5qe1ow"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "missing_values = []\n",
        "chunk_iter = pd.read_csv(\"https://bit.ly/3BPcobU\", encoding= 'unicode_escape', chunksize=500)\n",
        "for chunk in chunk_iter:\n",
        "    missing_values.append(chunk.isnull().sum())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory_footprints = []\n",
        "chunk_iter = pd.read_csv(\"https://bit.ly/3BPcobU\", encoding= 'unicode_escape', chunksize=500)\n",
        "for chunk in chunk_iter:\n",
        "    memory_footprints.append(chunk.memory_usage(deep=True).sum()/(1024*1024))"
      ],
      "metadata": {
        "id": "CoQjsKMO_fyp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://bit.ly/3BPcobU\", encoding= 'unicode_escape')\n",
        "print(df.info(memory_usage=\"deep\"))"
      ],
      "metadata": {
        "id": "A-w9MTSQ_qdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOEk3_YPdg6p"
      },
      "source": [
        "## Question 2: Selecting Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD1IqmzhcEd3"
      },
      "source": [
        "Now that we have a good sense of the missing values, let's get familiar with the column types before adding the data into SQLite.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "* Identify the types for each column.\n",
        "* Identify the numeric columns we can represent using more space efficient types.\n",
        "For text columns:\n",
        "* Analyze the unique value counts across all of the chunks to see if we can convert them to a numeric type.\n",
        "* See if we clean clean any text columns and separate them into multiple numeric columns without adding any overhead when querying.\n",
        "* Make your changes to the code from the last step so that the overall memory the data consumes stays under 10 megabytes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNgzuiStZ3WN"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def change_to_int(df, col_name):\n",
        "    # Get the minimum and maximum values\n",
        "  col_max = df[col_name].max()\n",
        "  col_min = df[col_name].min()\n",
        "    # Find the datatype\n",
        "  for dtype_name in ['int8', 'int16', 'int32', 'int64']:\n",
        "        # Check if this datatype can hold all values\n",
        "        if col_max <  np.iinfo(dtype_name).max and col_min > np.iinfo(dtype_name).min:\n",
        "            df[col_name] = df[col_name].astype(dtype_name)\n",
        "            break\n",
        "\n",
        "# Add you code below\n",
        "float_chunk = chunk.select_dtypes(include=['float64'])\n",
        "print(float_chunk.isnull().sum())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbvJjqohS5HO",
        "outputId": "900460f0-9dd8-422e-cf1c-6abf84d753e0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "investor_category_code    370\n",
            "investor_country_code     370\n",
            "investor_state_code       370\n",
            "investor_city             370\n",
            "raised_amount_usd          28\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_values = []\n",
        "chunk_iter = pd.read_csv(\"https://bit.ly/3BPcobU\", encoding= 'unicode_escape', chunksize=500)\n",
        "for chunk in chunk_iter:\n",
        "    unique_values.append(chunk.nunique())"
      ],
      "metadata": {
        "id": "zAtPP7OETuXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA2D7o4sds1t"
      },
      "source": [
        "## Question 3: Loading Chunks Into SQLite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptu8Xr25dvcF"
      },
      "source": [
        "Now we're in good shape to start exploring and analyzing the data. The next step is to load each chunk into a table in a SQLite database so we can query the full data set.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Create and connect to a new SQLite database file.\n",
        "2. Expand on the existing chunk processing code to export each chunk to a new table in the SQLite database.\n",
        "3. Query the table and make sure the data types match up to what you had in mind for each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH-j6fuXdu0v"
      },
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "conn = sqlite3.connect('crunchbase_investments.db')\n",
        "chunk_iter = pd.read_csv('https://bit.ly/3BPcobU', encoding= 'unicode_escape', chunksize=1000)\n",
        "for chunk in chunk_iter:\n",
        "    chunk.to_sql(\"crunchbase\", conn, if_exists='append', index=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt1jPEhseD8r"
      },
      "source": [
        "## Question 4: Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggt-hEebeJGr"
      },
      "source": [
        "Now that the data is in SQLite, we can use the pandas SQLite workflow we learned in the last lesson to explore and analyze startup investments. Remember that each row isn't a unique company, but a unique investment from a single investor. This means that many startups will span multiple rows.\n",
        "\n",
        "Use the pandas SQLite workflow to answer the following questions:\n",
        "\n",
        "* What proportion of the total amount of funds did the top 10% raise? What about the top 1%? Compare these values to the proportions the bottom 10% and bottom 1% raised.\n",
        "* Which category of company attracted the most investments?\n",
        "* Which investor contributed the most money (across all startups)?\n",
        "* Which investors contributed the most money per startup?\n",
        "* Which funding round was the most popular? Which was the least popular?\n",
        "\n",
        "Here are some ideas for further exploration:\n",
        "\n",
        "* Repeat the tasks in this project using stricter memory constraints (under 1 megabyte).\n",
        "* Clean and analyze the other Crunchbase data sets from the same GitHub repo.\n",
        "* Understand which columns the data sets share, and how the data sets are linked.\n",
        "* Create a relational database design that links the data sets together and reduces the overall disk space the database file consumes.\n",
        "\n",
        "Use pandas to populate each table in the database, create the appropriate indexes, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJo2f7N-ebgX"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}